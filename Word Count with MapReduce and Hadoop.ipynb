{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count with MapReduce and mrjob\n",
    "\n",
    "`mrjob` is a Python package that helps you write and run Hadoop Streaming jobs. It supports Amazon's Elastic MapReduce (EMR) and it also works with your own Hadoop cluster.  It has been released as an open-source framework by Yelp and we will use it to interface with Hadoop due to its legibility and ease of use with MapReduce tasks.  \n",
    "\n",
    "[mrjob docs](http://mrjob.readthedocs.org/en/latest/index.html) \n",
    "[mrjob tutorial](https://pythonhosted.org/mrjob/guides/quickstart.html) \n",
    "\n",
    "Some important features of ```mrjob```:\n",
    "\n",
    "* It can run jobs on EMR, your own Hadoop cluster, or locally (for testing).\n",
    "* It can be used to write multi-step jobs, where one map-reduce step feeds into the next. \n",
    "\n",
    "## Exercise 1: Use MapReduce to execute code across documents in multiple sub-directories \n",
    "\n",
    "### Simple Count\n",
    "\n",
    "Do a simple word count on the [Reuters 20 Newsgroups dataset](http://qwone.com/~jason/20Newsgroups/) click [here](http://qwone.com/~jason/20Newsgroups/20news-19997.tar.gz) to download.  If that is unavailable look [here](http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html). The beauty of MapReduce is that it will execute the code in all the subfolders within the specified directory. Cool!\n",
    "\n",
    "\n",
    "#### 1. Create a file `wordcounts.py` with the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "from mrjob.job import MRJob\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "class MRWordFreqCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        for word in line.split():\n",
    "            yield (word.strip(punctuation).lower(), 1)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFreqCount.run()\n",
    "\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create a mini version (to test code) of the dataset with these terminal commands: \n",
    "\n",
    "```bash\n",
    "    mkdir mini_20_newsgroups\n",
    "    mkdir mini_20_newsgroups/comp.windows.x\n",
    "    mkdir mini_20_newsgroups/rec.motorcycles\n",
    "    mkdir mini_20_newsgroups/sci.med\n",
    "    cp 20_newsgroups/comp.windows.x/663* mini_20_newsgroups/comp.windows.x\n",
    "    cp 20_newsgroups/rec.motorcycles/10311* mini_20_newsgroups/rec.motorcycles\n",
    "    cp 20_newsgroups/sci.med/5889* mini_20_newsgroups/sci.med\n",
    "    ```\n",
    "    \n",
    "#### 3. Execute the script from the terminal\n",
    "\n",
    "From the terminal, execute the file with the mini folder `mini_20_newsgroups` and the MapReduce will go through each folder in the directory and perform the word count.\n",
    "\n",
    "The results are stored in a text file called `mini20_results.txt` in the folder `print_outs`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python wordcounts.py ../data/mini_20_newsgroups > print_outs/mini20_results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: json and tokenization\n",
    "\n",
    "I want to take it a step further and use word tokenization to clean up my results. I have a json file containing NYT articles `articles.json`. I want to clean and lemmitize the text before I tokenize. I also want to count words by the times they appear in a given section of the newspaper. This is what I will call my topic.\n",
    "\n",
    "```Python\n",
    "import json\n",
    "from string import punctuation\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "sno = SnowballStemmer('english')\n",
    "\n",
    "class MRWordFreqCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        d = json.loads(line)\n",
    "        section = d['section_name']\n",
    "        for line in d['content']:\n",
    "            line = line.strip().encode('ascii','ignore').decode('utf-8').lower()\n",
    "            translator = line.maketrans('', '', punctuation)\n",
    "            new_line = line.translate(translator)\n",
    "            for word in word_tokenize(new_line):\n",
    "                    stemmed_wrd = sno.stem(word)\n",
    "                    # Word count by section name\n",
    "                    yield (\"{}_{}\".format(section, stemmed_wrd), 1)\n",
    "\n",
    "    def combiner(self, key, counts):\n",
    "        yield (key, sum(counts))\n",
    "\n",
    "    def reducer(self, key, counts):\n",
    "        yield (key, sum(counts))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFreqCount.run()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are sent to a text file called `json_results.txt` in the `print_outs` folder."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python wordcounts_json.py ../data/articles.json > print_outs/json_results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: What words are most associated with what topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement multiple mapper, combiner, and reducer functions in one MapReduce job. Then using the `MRStep` to tell `MRJob` the sequence to run them in. \n",
    "\n",
    "__topics_by_wordcount.py__\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import json\n",
    "from string import punctuation\n",
    "\n",
    "sno = SnowballStemmer('english')\n",
    "\n",
    "class WordCountByTopic(MRJob):\n",
    "\n",
    "    # Mapper 1: word count by section\n",
    "    def mapper1(self, _, line):\n",
    "        d = json.loads(line)\n",
    "        topic = d['section_name']\n",
    "        for art_text in d['content']:\n",
    "            art_text = art_text.strip().encode(\"ascii\", \"ignore\").decode('utf-8').lower()\n",
    "            translator = art_text.maketrans('', '', punctuation)\n",
    "            clean_text = art_text.translate(translator)\n",
    "            for word in word_tokenize(clean_text):\n",
    "                wrd = sno.stem(word)\n",
    "                yield (topic + \"_\" + wrd, 1)\n",
    "\n",
    "    def combiner1(self, key, counts):\n",
    "        yield (key, sum(counts))\n",
    "\n",
    "    def reducer1(self, key, counts):\n",
    "        yield (key, sum(counts))\n",
    "\n",
    "    # Mapper 2: For each word, what topic appears most frequent?\n",
    "    def mapper2(self, key, count):\n",
    "        topic, word = key.split('_')\n",
    "        yield word, (topic, count)\n",
    "\n",
    "    def combiner2(self, word, values):\n",
    "        yield word, max(values, key=lambda x: x[1])\n",
    "\n",
    "    def reducer2(self, word, values):\n",
    "        word_ = \"word: {}\".format(word)\n",
    "        topic_ = \"Topic: {}\".format(max(values, key=lambda x: x[1])[0])\n",
    "        yield word_, topic_\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper1,\n",
    "                   combiner=self.combiner1,\n",
    "                   reducer=self.reducer1),\n",
    "\n",
    "            MRStep(mapper=self.mapper2,\n",
    "                   combiner=self.combiner2,\n",
    "                   reducer=self.reducer2)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    WordCountByTopic.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are sent to a text file called `common_topic_results.txt` in the `print_outs` folder."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python topics_by_wordcount.py ../data/articles.json > print_outs/common_topic_results.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
